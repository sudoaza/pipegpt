#!/usr/bin/env python
import os, sys, re
import argparse
from easy_openai import *
from os.path import exists, dirname, join as joinpath, realpath
from time import sleep

parser = argparse.ArgumentParser(
  formatter_class=argparse.RawDescriptionHelpFormatter,
  description="""Integrate ChatGPT into your command pipelines.""", 
  epilog=f"""
By default it just passes the input to chatGPT.

    $ echo "The moons of Mars are called" | pipegpt
    > Phobos and Deimos.

You can pass inline prompts to prepend to the input.

    cat words.txt | pipegpt "Which of these are fruits? One per line."
    > Coconut
    > Mandarin

Or you can choose prompts from templates. Also present results as a unique list (-l).

    cat passwords.txt | pipegpt -l password_suggest

Add your own tasks.

    echo "Tell me a joke about " > {joinpath( dirname(realpath(__file__)), "tasks/joke_about.txt" )}
    echo "chatbots" | pipegpt joke_about
""")
parser.add_argument('task', help='The task to execute. Known prompt to prepend to input.', nargs="?", default="")
parser.add_argument('-l','--list', help='Results are an unsorted list, get alternatives.', action="store_true")
parser.add_argument('-rl','--reply-len', help='Maximum reply length in characters. Prompt plus reply length are contrained to max length 4097.', type=int, default=512)
parser.add_argument('-m','--model', help="Model to use '3' (default) for 'gpt-3.5-turbo' or '4' for 'gpt-4'.", default="3")
parser.add_argument('-n','--number', help="Minimum number of lines to return. Combine with -l/--list.", type=int, default=10)
parser.add_argument('-b','--batch', help="Batch input longer than 4k and concat output.", action="store_true")

args = parser.parse_args()

if args.model == "3":
  args.model = "gpt-3.5-turbo"

if args.model == "4":
  args.model = "gpt-4"

def task_prompt(task):
  if not task or task == "":
    return ""
  
  default_path = joinpath( dirname(realpath(__file__)), "tasks", f"{task}.txt" )
  if exists( default_path ):
    task = open(default_path).read()

  return task

# List that drops duplicates
class UniqList(list):
  def __init__(self, glue="\n"):
    self.glue = glue

  def __str__(self):
    return self.glue.join(self)

  def extend(self, _list):
    if type(_list) == str:
      _list = _list.split(self.glue)
    super().extend([el for el in _list if el not in self and el.strip() != ""])

  def append(self, el):
    if el not in self and el.strip() != "":
      super().append(el)

  def __setitem__(self, index, item):
    if item not in self and item.strip() != "":
      super().__setitem__(index, item)

def run_task(task, input):
  if "%INPUT%" in task:
    task = task.replace("%INPUT%",input)
    input = ""

  if len(task + input) > PROMPT_LEN:
    print("Input",len(task),"is longer than supported max length ",PROMPT_LEN,", truncating to ", PROMPT_LEN, ". Consider using -b/--batch", file=sys.stderr)
    task = task[:PROMPT_LEN]

  return ask_chat(input, task)

def ask_chat(input, task):
  if args.list:
    results = UniqList()
    for _ in range(min(100, args.number)):
      _, completion = chat_complete(input, system=task)
      for choice in completion.choices:
        results.extend( clean_list(choice.text) )

      if len(results) > args.number:
        break

    return str(results)

  else:
    _, completion = chat_complete(input, system=task)
    return completion

def clean_list(s_list):
  # It's a comma separated list as a sentence
  if re.match(r"([^,]+, ){2,}[^,]+\.",s_list):
    s_list = re.sub(", ", "\n", s_list.rstrip("\n .,"))

  # It's a semicolon separated list as a sentence
  if re.match(r"([^;]+; ){2,}[^;]+\.",s_list):
    s_list = re.sub("; ", "\n", s_list.rstrip("\n .,"))

  # Remove bullets
  return re.sub(r"([0-9]+\.|-) ","",s_list)

MAX_BACKOFF = 15
def try_ask(*args):
  backoff = 3
  err = None
  while backoff < MAX_BACKOFF:
    try:
      return ask(*args)
    except (openai.error.ServiceUnavailableError, openai.error.RateLimitError) as e:
      print("Unavailable. Sleep", backoff, file=sys.stderr)
      sleep(backoff)
      backoff *= 2
      err = e
  raise err

def get_whole_chunk(message, size):
  # Size is fine, return whole
  if len(message) <= size:
    return message

  # Cut by last new line or dot that fits size
  cut_at = max( message.rfind("\n", 0, size), message.rfind(".", 0, size) )

  # Dont waste more than half the size
  if cut_at < size / 2:
    return message[:size]

  return message[:cut_at+1]

def pipegpt(message, task):
  INPUT_MAXLEN = PROMPT_LEN - len(task) + 7*int("%INPUT%" in task)

  if len(message) > INPUT_MAXLEN and args.batch:
    pending = message
    response = ""
    chunk = "nada"
    while len(chunk) > 0 and len(pending) > 0:
      chunk = get_whole_chunk(pending, INPUT_MAXLEN - 2)
      pending = pending[len(chunk):]
      response += "\n"*int(bool(len(response))) + run_task(task, chunk + "\n\n")
    return response

  return run_task(task, message)

# This will be model dependant
MAXLEN = 4097
REPLY_LEN = args.reply_len
PROMPT_LEN = MAXLEN - REPLY_LEN

def main():
  message = sys.stdin.read()
  task = task_prompt(args.task)

  if type(message) == list:
    message = "".join(message)

  print(pipegpt(message, task))

if __name__ == '__main__':
  main()